{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "Adaptive Boosting. Implementation with trees as weak learners\n",
    "\n",
    "**Mathematical Background:**  \n",
    "Goal is to estimate the paremeters of weak models and the weights to combine the models' decisions to produce a final prediction:  \n",
    "  \n",
    "<center style=\"margin: 20px;\">$F(x) = \\sum_{k=1}^{N}a_k\\phi(x;\\theta_k)$</center>\n",
    "\n",
    "where $N$ is the number of samples and $\\phi(x;\\theta)$ is a weak classifier, by minimizing the cost function $\\sum_{k=1}^{N}\\exp(-y_iF(x_i))$ in terms of $a_k$ and $\\theta_k$\n",
    "\n",
    "Since this problem is generally hard, we optimize each weak model of the partial sum $F_m(x)$ assuming optimality for previous terms:\n",
    "\n",
    "<center style=\"margin: 20px;\">$F_{m}(x) = F_{m-1}(x) + a_m\\phi(x;\\theta_m)$</center>\n",
    "\n",
    "Key point is that when optimizing $\\phi(x;\\theta_m)$ in terms of $\\theta_m$, the samples $x_i$ are weighted according to the ability of the classifier of the previous step to classify them correctly. The weights are $w_i = \\exp(-y_iF_{m-1}(x_i))$.  \n",
    "\n",
    "Given these weights, the current classifier's objective is to minimize the classification error, weighting each sample accordingly. For example, a classification tree can be used as weak learner using sample weights to calculate class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T20:51:53.827579Z",
     "start_time": "2020-03-27T20:51:53.798578Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width: 98%}</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>.container {width: 98%}</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T20:51:54.746760Z",
     "start_time": "2020-03-27T20:51:54.170576Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T00:01:08.179202Z",
     "start_time": "2020-03-28T00:01:06.556202Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from functools import reduce\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "from classification_tree import ClassificationTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T00:01:36.338723Z",
     "start_time": "2020-03-28T00:01:35.055722Z"
    }
   },
   "outputs": [],
   "source": [
    "class AdaBoost(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Adaptive Boosting learner based on classification trees\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    fit(X, y)\n",
    "        Iteratively adds and fits weak learners on data by updating sample weights\n",
    "        according to classification error of previous model\n",
    "\n",
    "    predict(X)\n",
    "        Returns predictions for the input samples\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.alphas = None  # weights used to combine weak learners\n",
    "        self.learners = None\n",
    "        self.p_list = None\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, max_iters: int = 10, **kwargs) -> AdaBoost:\n",
    "        \"\"\"\n",
    "        Iteratively adds and fits weak learners on data by updating sample weights\n",
    "        according to classification error of previous model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray\n",
    "            Array of training samples with shape (n_samples, n_features)\n",
    "\n",
    "        y : numpy.ndarray\n",
    "            Array of training targets with shape (n_samples,)\n",
    "\n",
    "        max_iters : int\n",
    "            Number of boosting iterations\n",
    "        \"\"\"\n",
    "\n",
    "        weights = np.ones(X.shape[0]) / X.shape[0]\n",
    "        m = 0\n",
    "        self.learners = [None] * max_iters  # arrayholding all learners\n",
    "        self.alphas = [None] * max_iters  # array holding weights\n",
    "        self.p_list = [None] * max_iters  # array holding classification error in each iteration\n",
    "\n",
    "        while True:\n",
    "            clf = ClassificationTree(max_depth=4, min_leaf_samples=1, min_delta_impurity=0.0)\n",
    "            clf = clf.fit(X, y, sample_weights=weights, **kwargs)\n",
    "            self.learners[m] = clf\n",
    "\n",
    "            y_pred = clf.predict(X)\n",
    "            P_m = (((1 - y * y_pred) > 0).astype(int) * weights).sum()\n",
    "            self.p_list[m] = P_m\n",
    "\n",
    "            a_m = (1 / 2) * np.log((1 - P_m) / P_m)\n",
    "            self.alphas[m] = a_m\n",
    "\n",
    "            weights = weights * np.exp(-y * a_m * y_pred)\n",
    "            weights = weights / weights.sum()\n",
    "\n",
    "            m += 1\n",
    "            if m == max_iters:\n",
    "                break\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: np.ndarray):\n",
    "        \"\"\"\n",
    "        Returns predictions for the input samples\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray\n",
    "            Array of testing samples with shape (n_samples, n_features)\n",
    "        \"\"\"\n",
    "\n",
    "        if self.alphas is None:\n",
    "            raise ValueError(\"Model not fitted. Call fit() method first\")\n",
    "\n",
    "        return np.sign(np.array([a * clf.predict(X) for a, clf in zip(self.alphas, self.learners)]).T.sum(axis=1))\n",
    "\n",
    "    def score(self, X, y, **kwargs):\n",
    "        return accuracy_score(y, self.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a simple dataset with 2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T00:01:40.004928Z",
     "start_time": "2020-03-28T00:01:38.539753Z"
    }
   },
   "outputs": [],
   "source": [
    "data = datasets.load_breast_cancer()\n",
    "X = data[\"data\"]\n",
    "y = data[\"target\"].astype(np.float64)\n",
    "y[y == 0] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Over)fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T00:12:24.064045Z",
     "start_time": "2020-03-28T00:11:59.135813Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoost()"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada = AdaBoost()\n",
    "ada.fit(X, y, max_iters=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T00:13:03.213296Z",
     "start_time": "2020-03-28T00:13:01.735297Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[212,   0],\n",
       "       [  0, 357]], dtype=int64)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = ada.predict(X)\n",
    "confusion_matrix(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T00:10:01.645695Z",
     "start_time": "2020-03-28T00:09:08.321022Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9578481602235677"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(ada, X, y, cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare with a simple decision tree\n",
    "With the same parameters as the weak learner of AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T00:08:23.688709Z",
     "start_time": "2020-03-28T00:08:20.894702Z"
    }
   },
   "outputs": [],
   "source": [
    "tree = ClassificationTree(max_depth=4, min_leaf_samples=1, min_delta_impurity=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T00:08:39.428471Z",
     "start_time": "2020-03-28T00:08:26.965444Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9192206179164726"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(tree, X, y, cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T23:47:23.200285Z",
     "start_time": "2020-03-27T23:47:21.369280Z"
    }
   },
   "source": [
    "We can see that the boosted method performed much better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
